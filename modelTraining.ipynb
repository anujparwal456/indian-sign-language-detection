{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8996 images belonging to 36 classes.\n",
      "Found 2989 images belonging to 36 classes.\n",
      "Found 3031 images belonging to 36 classes.\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anujp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\anujp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 1s/step - accuracy: 0.5075 - loss: 1.9517 - val_accuracy: 0.3757 - val_loss: 2.2107 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3127s\u001b[0m 11s/step - accuracy: 0.8647 - loss: 0.4503 - val_accuracy: 0.9575 - val_loss: 0.1590 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 423ms/step - accuracy: 0.9200 - loss: 0.3052 - val_accuracy: 0.9913 - val_loss: 0.0488 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 407ms/step - accuracy: 0.9457 - loss: 0.2038 - val_accuracy: 0.9920 - val_loss: 0.0439 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 423ms/step - accuracy: 0.9541 - loss: 0.1698 - val_accuracy: 0.9358 - val_loss: 0.2088 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 422ms/step - accuracy: 0.9606 - loss: 0.1461 - val_accuracy: 0.9950 - val_loss: 0.0346 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 423ms/step - accuracy: 0.9651 - loss: 0.1249 - val_accuracy: 0.9839 - val_loss: 0.0513 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 458ms/step - accuracy: 0.9685 - loss: 0.1124 - val_accuracy: 0.9950 - val_loss: 0.0207 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 418ms/step - accuracy: 0.9705 - loss: 0.0904 - val_accuracy: 0.9963 - val_loss: 0.0232 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 410ms/step - accuracy: 0.9741 - loss: 0.0875 - val_accuracy: 0.9926 - val_loss: 0.0328 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 417ms/step - accuracy: 0.9791 - loss: 0.0732 - val_accuracy: 0.9930 - val_loss: 0.0298 - learning_rate: 0.0010\n",
      "Epoch 12/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 414ms/step - accuracy: 0.9789 - loss: 0.0698 - val_accuracy: 0.9873 - val_loss: 0.0408 - learning_rate: 0.0010\n",
      "Epoch 13/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - accuracy: 0.9794 - loss: 0.0649\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 418ms/step - accuracy: 0.9794 - loss: 0.0649 - val_accuracy: 0.9515 - val_loss: 0.1744 - learning_rate: 0.0010\n",
      "Epoch 14/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 442ms/step - accuracy: 0.9793 - loss: 0.0634 - val_accuracy: 0.9926 - val_loss: 0.0273 - learning_rate: 5.0000e-04\n",
      "Epoch 15/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 415ms/step - accuracy: 0.9870 - loss: 0.0422 - val_accuracy: 0.9983 - val_loss: 0.0129 - learning_rate: 5.0000e-04\n",
      "Epoch 16/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 408ms/step - accuracy: 0.9906 - loss: 0.0329 - val_accuracy: 0.9980 - val_loss: 0.0129 - learning_rate: 5.0000e-04\n",
      "Epoch 17/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 417ms/step - accuracy: 0.9886 - loss: 0.0396 - val_accuracy: 0.9983 - val_loss: 0.0156 - learning_rate: 5.0000e-04\n",
      "Epoch 18/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 412ms/step - accuracy: 0.9886 - loss: 0.0391 - val_accuracy: 0.9977 - val_loss: 0.0171 - learning_rate: 5.0000e-04\n",
      "Epoch 19/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 516ms/step - accuracy: 0.9893 - loss: 0.0370 - val_accuracy: 0.9977 - val_loss: 0.0141 - learning_rate: 5.0000e-04\n",
      "Epoch 20/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.9882 - loss: 0.0344\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 409ms/step - accuracy: 0.9882 - loss: 0.0344 - val_accuracy: 0.9957 - val_loss: 0.0207 - learning_rate: 5.0000e-04\n",
      "Epoch 21/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 506ms/step - accuracy: 0.9912 - loss: 0.0278 - val_accuracy: 0.9983 - val_loss: 0.0139 - learning_rate: 2.5000e-04\n",
      "Epoch 22/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 415ms/step - accuracy: 0.9917 - loss: 0.0277 - val_accuracy: 0.9973 - val_loss: 0.0160 - learning_rate: 2.5000e-04\n",
      "Epoch 23/25\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 420ms/step - accuracy: 0.9952 - loss: 0.0203 - val_accuracy: 0.9987 - val_loss: 0.0130 - learning_rate: 2.5000e-04\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "Test Set Evaluation:\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 516ms/step - accuracy: 0.9968 - loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.74%\n",
      "\n",
      "Training samples (with augmentation):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 141\u001b[39m\n\u001b[32m    138\u001b[39m     plt.show()\n\u001b[32m    140\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining samples (with augmentation):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m \u001b[43mvisualize_augmentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mValidation samples (no augmentation):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m visualize_augmentations(val_generator)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mvisualize_augmentations\u001b[39m\u001b[34m(generator, num_images)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualize_augmentations\u001b[39m(generator, num_images=\u001b[32m5\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     x, y = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m()\n\u001b[32m    132\u001b[39m     plt.figure(figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_images):\n",
      "\u001b[31mAttributeError\u001b[39m: 'DirectoryIterator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import splitfolders\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Dataset Preprocessing and Splitting (No changes needed here unless your folder structure is different)\n",
    "# --------------------------------------------\n",
    "input_folder = \"C:/Users/crite/Downloads/Final_Sign_Dataset\"\n",
    "output_folder = \"preprocessed_alphaDigi_dataset\"\n",
    "\n",
    "\n",
    "# Step 2: Create Data Generators with Preprocessing\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Training generator with more aggressive augmentation (within reason for sign language)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.05,  # Add horizontal shift\n",
    "    height_shift_range=0.05, # Add vertical shift\n",
    "    zoom_range=0.05,\n",
    "    shear_range=0.03,        # Add shear\n",
    "    horizontal_flip=False,\n",
    "    brightness_range=[0.6, 1.4], # More variation in brightness\n",
    "    fill_mode='constant',\n",
    "    cval=0                      # Fill with black (often better than white for CNNs)\n",
    ")\n",
    "\n",
    "# Validation and Test generators (only rescaling)\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Common parameters\n",
    "target_size = (128, 128)\n",
    "color_mode = 'rgb'  # Experiment with RGB - might capture more subtle features\n",
    "batch_size = 32\n",
    "class_mode = 'categorical'\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(output_folder, 'train'),\n",
    "    target_size=target_size,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(output_folder, 'val'),\n",
    "    target_size=target_size,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode,\n",
    "    shuffle=False # No need to shuffle validation data\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(output_folder, 'test'),\n",
    "    target_size=target_size,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode,\n",
    "    shuffle=False # Important for accurate evaluation\n",
    ")\n",
    "\n",
    "# Step 3: Improved Model Building\n",
    "# ----------------------\n",
    "\n",
    "print(\"training\")\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'), \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Use a smaller learning rate initially and add ReduceLROnPlateau\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Training with Enhanced Callbacks\n",
    "# ------------------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1) # Increased patience\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1) # Reduce LR if val_loss plateaus\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25, # Increased epochs - early stopping will handle overtraining\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Step 5: Final Evaluation\n",
    "# ------------------------\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('sign_language_alphaDigit_model.h5')\n",
    "\n",
    "# Optional: Visualize Preprocessing\n",
    "# ---------------------------------\n",
    "def visualize_augmentations(generator, num_images=5):\n",
    "    next(generator)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(x[i].squeeze())\n",
    "        plt.title(f\"Label: {list(generator.class_indices.keys())[np.argmax(y[i])]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nTraining samples (with augmentation):\")\n",
    "visualize_augmentations(train_generator)\n",
    "\n",
    "print(\"\\nValidation samples (no augmentation):\")\n",
    "visualize_augmentations(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
